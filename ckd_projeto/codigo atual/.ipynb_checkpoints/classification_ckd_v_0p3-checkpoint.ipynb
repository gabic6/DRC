{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%-----------------------------------------------------------------------------------------------------------  \n",
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import pygmo as pg\n",
    "import pylab as pl\n",
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "\n",
    "from sklearn.model_selection import (GridSearchCV, KFold, cross_val_predict, \n",
    "                                     TimeSeriesSplit, cross_val_score, \n",
    "                                     LeaveOneOut, KFold, StratifiedKFold,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     cross_val_predict,train_test_split)\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "#from sklearn.metrics.regression import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "#from sklearn.metrics.classification import accuracy_score, f1_score, precision_score\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, MaxAbsScaler, Normalizer, StandardScaler, MaxAbsScaler, FunctionTransformer, QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.gaussian_process import  GaussianProcessClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVR, LinearSVR, SVC\n",
    "from sklearn.linear_model import ElasticNet, Ridge, PassiveAggressiveRegressor, LogisticRegression, BayesianRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from xgboost import  XGBRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import (RBF, Matern, RationalQuadratic,\n",
    "                                              ExpSineSquared, DotProduct,\n",
    "                                              ConstantKernel)\n",
    "\n",
    "from ELM import ELMClassifier\n",
    "#import re  \n",
    "import os\n",
    "#from sklearn.gaussian_process import GaussianProcess\n",
    "#from catboost import Pool, CatBoostRegressor\n",
    "#from pyearth import Earth as MARS\n",
    "#from sklearn.ensemble import StackingRegressor\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "#from sklearn.kernel_approximation import RBFSampler,SkewedChi2Sampler\n",
    "\n",
    "#from util.ELM import  ELMRegressor, ELMRegressor\n",
    "#from util.MLP import MLPRegressor as MLPR\n",
    "#from util.RBFNN import RBFNNRegressor, RBFNN\n",
    "#from util.LSSVR import LSSVR\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, MaxAbsScaler, Normalizer, StandardScaler, MaxAbsScaler, FunctionTransformer, QuantileTransformer\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "#from utils.confusion_matrix_pretty_print import plot_confusion_matrix_from_data\n",
    "#from ds_utils.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.models import Sequential\n",
    "import keras.layers\n",
    "\n",
    "#%%----------------------------------------------------------------------------\n",
    "def RMSE(y, y_pred):\n",
    "    y, y_pred = np.array(y).ravel(), np.array(y_pred).ravel()\n",
    "    error = y -  y_pred    \n",
    "    return np.sqrt(np.mean(np.power(error, 2)))\n",
    "\n",
    "def MAPE(y_true, y_pred):    \n",
    "  y_true, y_pred = np.array(y_true).ravel(), np.array(y_pred).ravel()\n",
    "  return np.mean(np.abs(y_pred - y_true)/np.abs(y_true))*100\n",
    "  #return RMSE(y, y_pred)\n",
    "  \n",
    "#%%----------------------------------------------------------------------------\n",
    "\n",
    "#pd.options.display.float_format = '{:20,.3f}'.format\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys, getopt\n",
    "program_name = sys.argv[0]\n",
    "arguments = sys.argv[1:]\n",
    "count = len(arguments)\n",
    "\n",
    "# print (\"This is the name of the script: \", program_name)\n",
    "# print (\"Number of arguments: \", len(arguments))\n",
    "# print (\"The arguments are: \" , arguments)\n",
    "\n",
    "if len(arguments)>0:\n",
    "  if arguments[0]=='-r':\n",
    "    run0 = int(arguments[1])\n",
    "    n_runs = run0+1\n",
    "  else:\n",
    "    run0, n_runs = 0,1\n",
    "else:\n",
    "  run0, n_runs = 0,1\n",
    "\n",
    "#%%----------------------------------------------------------------------------   \n",
    "def build_model(train_dataset):\n",
    "  model = Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=[len(train_dataset)]),\n",
    "    layers.Dropout(0.1),\n",
    "    # layers.Dense(64, activation='relu'),\n",
    "    \n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "#%%----------------------------------------------------------------------------   \n",
    "basename='evo_ml_'\n",
    "\n",
    "from read_data import *\n",
    "datasets = [\n",
    "            read_data_cenario('cenario1.csv'),\n",
    "            # read_data_cenario('cenario2.csv'),\n",
    "            # read_data_cenario('cenario3.csv'),\n",
    "            # read_data_cenario('cenario4.csv')\n",
    "          ]\n",
    "#%%----------------------------------------------------------------------------   \n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "from scipy.stats import uniform, randint\n",
    "from drc_cenarios import cenario1,cenario2,cenario3,cenario4\n",
    "\n",
    "cenarios = [cenario1,cenario2,cenario3,cenario4] #keys\n",
    "pop_size    = 20\n",
    "max_iter    = 40\n",
    "n_splits    = 3\n",
    "scoring     = 'neg_mean_squared_error'\n",
    "scoring     = 'neg_root_mean_squared_error'\n",
    "for run in range(run0, n_runs):\n",
    "    random_seed=run+100\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    estimators=[\n",
    "        # #\n",
    "        # # XGB\n",
    "        # #\n",
    "        # (\n",
    "        #  #\n",
    "        #  # acronym\n",
    "        #  #\n",
    "        #  'XGB',\n",
    "        #  #\n",
    "        #  # distributions\n",
    "        #  #\n",
    "        #  dict(  n_estimators=randint(low=1, high=1e3),\n",
    "        #         max_depth=randint(low=1, high=10),\n",
    "        #         learning_rate=uniform(loc=0, scale=1),\n",
    "        #         gamma=uniform(loc=0, scale=1),\n",
    "        #         reg_alpha=uniform(loc=0, scale=1),\n",
    "        #         reg_lambda=uniform(loc=0, scale=1),\n",
    "        #         #degree=uniform(loc=1, scale=5),\n",
    "        #                            ),\n",
    "        #  #\n",
    "        #  # estimator\n",
    "        #  #\n",
    "        #  XGBClassifier(random_state=random_seed)\n",
    "        #  ),\n",
    "        #\n",
    "        # LinearRegression\n",
    "        #\n",
    "        # (\n",
    "        # #\n",
    "        # # acronym\n",
    "        # #\n",
    "        # 'LR',\n",
    "        # #\n",
    "        # # distributions\n",
    "        # #\n",
    "        # dict( \n",
    "        #      C=uniform(loc=1, scale=1e3),\n",
    "        #      l1_ratio=uniform(loc=0, scale=1),\n",
    "        #     ),\n",
    "        # #\n",
    "        # # estimator\n",
    "        # #\n",
    "        # LogisticRegression(random_state=random_seed)\n",
    "        # ),\n",
    "        \n",
    "        # (\n",
    "        # #\n",
    "        # # acronym\n",
    "        # #\n",
    "        # 'SVC',\n",
    "        # #\n",
    "        # # distributions\n",
    "        # #\n",
    "        # dict( \n",
    "        #       C=uniform(loc=1, scale=1e3),\n",
    "        #       gamma=uniform(loc=0.001, scale=100),\n",
    "        #     ),\n",
    "        # #\n",
    "        # # estimator\n",
    "        # #\n",
    "        # SVC(kernel='rbf', max_iter=1000, random_state=random_seed)\n",
    "        # ),\n",
    "        \n",
    "        # (\n",
    "        # #\n",
    "        # # acronym\n",
    "        # #\n",
    "        # 'KNN',\n",
    "        # #\n",
    "        # # distributions\n",
    "        # #\n",
    "        # dict( \n",
    "        #      n_neighbors=randint(low=1, high=15),\n",
    "        #      p=randint(low=1, high=3),\n",
    "        #     ),\n",
    "        # #\n",
    "        # # estimator\n",
    "        # #\n",
    "        # KNeighborsClassifier()\n",
    "        # ),\n",
    "        \n",
    "        #  (\n",
    "        # # #\n",
    "        # # # acronym\n",
    "        # # #\n",
    "        #  'MLP',\n",
    "        #  #\n",
    "        #  # distributions\n",
    "        #  #\n",
    "        #  dict( \n",
    "        #       hidden_layer_sizes=randint(low=1, high=100),\n",
    "        #      ),\n",
    "        #  #\n",
    "        #  # estimator\n",
    "        #  #\n",
    "        #  MLPClassifier(activation='relu', random_state=random_seed)\n",
    "        #  ),\n",
    "        \n",
    "        # (\n",
    "        # #\n",
    "        # # acronym\n",
    "        # #\n",
    "        # 'ELM',\n",
    "        # #\n",
    "        # # distributions\n",
    "        # #\n",
    "        # dict( \n",
    "        #      n_hidden=randint(low=10, high=300),\n",
    "        #     ),\n",
    "        # #\n",
    "        # # estimator\n",
    "        # #\n",
    "        # ELMClassifier(activation_func='identity', random_state=random_seed)\n",
    "        # ),\n",
    "        \n",
    "#        (\n",
    "#        #\n",
    "#        # acronym\n",
    "#        #\n",
    "#        'GPC',\n",
    "#        #\n",
    "#        # distributions\n",
    "#        #\n",
    "#        dict( \n",
    "#              \n",
    "#            ),\n",
    "#        #\n",
    "#        # estimator\n",
    "#        #\n",
    "#        GaussianProcessClassifier(optimizer='fmin_l_bfgs_b', random_state=random_seed)\n",
    "#        ),\n",
    "        \n",
    "        ]\n",
    "\n",
    "    cont=0 \n",
    "    for dataset in datasets:#[:1]:\n",
    "        dr=dataset['name'].replace(' ','_').replace(\"'\",\"\").lower()\n",
    "        path='./pkl_'+dr+'/'\n",
    "        os.system('mkdir  '+path)\n",
    "        \n",
    "        #for (target,y_train,y_test) in zip(dataset['target_names'], dataset['y_train'], dataset['y_test']):                        \n",
    "        for tk, tn in enumerate(dataset['target_names']):\n",
    "            print (tk, tn)\n",
    "            target                          = dataset['target_names'][tk]\n",
    "            y      , y_test                 = dataset['y_train'][tk], dataset['y_test'][tk]\n",
    "            dataset_name, X      , X_test   = dataset['name'], dataset['X_train'], dataset['X_test']\n",
    "            n_samples_train, n_features     = dataset['n_samples'], dataset['n_features']\n",
    "            task, normalize                 = dataset['task'], dataset['normalize']\n",
    "            n_samples_test                  = len(y_test)\n",
    "            np.random.seed(random_seed)\n",
    "\n",
    "            s=''+'\\n'\n",
    "            s+='='*80+'\\n'\n",
    "            s+='Dataset                    : '+dataset_name+' -- '+target+'\\n'\n",
    "            s+='Number of training samples : '+str(n_samples_train) +'\\n'\n",
    "            s+='Number of testing  samples : '+str(n_samples_test) +'\\n'\n",
    "            s+='Number of features         : '+str(n_features)+'\\n'\n",
    "            s+='Normalization              : '+str(normalize)+'\\n'\n",
    "            s+='Task                       : '+str(dataset['task'])+'\\n'\n",
    "            s+='Reference                  : '+str(dataset['reference'])+'\\n'\n",
    "            s+='='*80\n",
    "            s+='\\n'            \n",
    "            \n",
    "            print(s)     \n",
    "            \n",
    "            X = pd.DataFrame(X).fillna(0).values\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)  \n",
    "            model = build_model(cenarios[cont])\n",
    "            callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=20)\n",
    "            model.fit(X_train, y_train, epochs=150, batch_size=1024,\n",
    "             verbose=1,callbacks=callback)\n",
    "            predictions = model.predict_classes(X_test)\n",
    "            model.summary()\n",
    "            cont+=1\n",
    "                # clf.fit(X_train, y_train)                  \n",
    "                # y_pred = clf.predict(X_test)              \n",
    "                # columns = [str(i) for i in np.unique(y_test)]\n",
    "                #plot_confusion_matrix_from_data(y_test, y_pred, columns, figsize=[4, 4],)\n",
    "                #plot_confusion_matrix(y_test, y_pred, columns, figsize=[4, 4],)\n",
    "                \n",
    "                # print(acronym, classification_report(y_test, y_pred))\n",
    "#%%----------------------------------------------------------------------------   \n",
    "\n",
    "            # from xgboost import plot_importance\n",
    "            # model = clf.best_estimator_\n",
    "            # plot_importance(model)\n",
    "\n",
    "#%%----------------------------------------------------------------------------   "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
